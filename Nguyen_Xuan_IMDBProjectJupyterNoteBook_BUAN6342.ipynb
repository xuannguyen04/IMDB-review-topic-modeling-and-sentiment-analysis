{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdec470-9c8e-42a4-a98b-fc68d5629089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:06.506902Z",
     "start_time": "2025-10-27T19:52:06.498672Z"
    }
   },
   "outputs": [],
   "source": [
    "__author__='Xuan Nguyen'\n",
    "__NetID__='XDN240000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4736bf-39dd-4db5-8292-9cbd4b7234c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:36:30.640245Z",
     "start_time": "2025-10-27T15:36:30.637325Z"
    }
   },
   "source": [
    "# NLP PROJECT — IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ff826b-5283-4c65-b00e-1ae97d1824e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:06.515493Z",
     "start_time": "2025-10-27T19:52:06.512819Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import important libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9970e1f6-964a-41d2-bfbc-3dcc96540277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:06.531992Z",
     "start_time": "2025-10-27T19:52:06.522374Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/huy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/huy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/huy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download resources if missing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "custom_stopwords = stopwords.union({\"film\", \"films\", \"movie\", \"movies\", \"one\", \"make\", \"see\", \"get\", \"time\",\n",
    "    \"watch\", \"watching\", \"seen\", \"good\", \"like\", \"really\",\n",
    "    \"even\", \"dont\", \"would\", \"know\", \"think\", \"im\", \"people\", \"guy\", \"thing\",\n",
    "    \"show\", \"story\", \"character\", \"characters\", \"life\", \"well\", \"also\",\n",
    "    \"plot\", \"scene\", \"scenes\", \"acting\",\"many\",\"much\",\"way\",\"didnt\",\"go\",\"say\",\"something\",\"could\",\"made\"})\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add666c6-9bfa-407b-be37-8a3dd22098cf",
   "metadata": {},
   "source": [
    "## PART 1 — TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f411b38-0c42-48d5-bcad-83fe160c116d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:13.806371Z",
     "start_time": "2025-10-27T19:52:06.541775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of cleaned data\n",
      "0    liked summerslam due look arena curtain look o...\n",
      "1    television show appeal quite different kind fa...\n",
      "2    quickly get major chase ever increasing destru...\n",
      "3    jane austen definitely approve onegwyneth palt...\n",
      "4    expectation somewhat high went thought steve c...\n",
      "Name: clean_review, dtype: object\n",
      "Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# --- Basic text cleaning ---\n",
    "# Initialize stopwords and lemmatizer ONCE\n",
    "\n",
    "\n",
    "def base_clean(text):\n",
    "    \"\"\"Base cleaning function to avoid duplication\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text.lower())\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = base_clean(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in custom_stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df = data.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# --- Feature extraction ---\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=5)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5)\n",
    "count_data = count_vectorizer.fit_transform(df['clean_review'])\n",
    "tfidf_data = tfidf_vectorizer.fit_transform(df['clean_review'])\n",
    "print(\"Preview of cleaned data\")\n",
    "print(df['clean_review'].head())\n",
    "print(\"Preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b867b-02c7-4569-95fd-a05d839401ae",
   "metadata": {},
   "source": [
    "## PART 2 — SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd98a01-1654-4f58-9497-92bde49840c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:13.932930Z",
     "start_time": "2025-10-27T19:52:13.832023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8740\n",
      "Precision: 0.8523\n",
      "Recall:    0.9051\n",
      "F1-score:  0.8779\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.84      0.87       999\n",
      "    positive       0.85      0.91      0.88      1001\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.88      0.87      0.87      2000\n",
      "weighted avg       0.88      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Splitting data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_data, df['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, pos_label='positive')\n",
    "rec = recall_score(y_test, y_pred, pos_label='positive')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7102f-b6fd-4b59-99b2-42cd8bd540ad",
   "metadata": {},
   "source": [
    "## PART 3 — TOPIC MODELING (LDA + NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea977f6-6327-4346-a7e9-dc09701f02b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:35.939517Z",
     "start_time": "2025-10-27T19:52:13.939164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================== LDA Topics ==============================\n",
      "Topic #                            Human Label                                                               Top Words\n",
      "Topic 1                Overall Movie Judgments         bad, first, ever, great, look, better, actor, funny, never, lot\n",
      "Topic 2 Praising Performances, Music, & Comedy great, best, song, music, comedy, year, love, first, still, performance\n",
      "Topic 3         Critiquing Male Roles & Horror            man, great, role, he, play, horror, end, first, never, actor\n",
      "Topic 4                     War & Action Films             man, world, two, war, take, first, american, end, come, new\n",
      "Topic 5          Drama, Romance, & Adaptations   love, work, director, book, woman, performance, two, man, great, play\n",
      "\n",
      "\n",
      "============================== NMF Topics ==============================\n",
      "Topic #                                  Human Label                                                                          Top Words\n",
      "Topic 1             Analyzing Direction & Characters                    man, woman, end, two, little, take, look, director, work, young\n",
      "Topic 2   Strongly Negative Reviews (Waste of Money)             bad, worst, ever, waste, awful, money, stupid, terrible, horrible, ive\n",
      "Topic 3       Praising Comedy, Music, & Performances             great, love, funny, best, comedy, actor, music, year, performance, saw\n",
      "Topic 4            Discussing Book/Novel Adaptations book, read, version, novel, adaptation, jane, reading, based, disappointed, better\n",
      "Topic 5 TV Series Fan Discussion (Seasons, Episodes)           series, episode, tv, season, first, show, original, new, television, fan\n"
     ]
    }
   ],
   "source": [
    "#Extrating topics\n",
    "N_TOPIC = 5\n",
    "\n",
    "# --- LDA ---\n",
    "lda = LatentDirichletAllocation(n_components=N_TOPIC, random_state=42, learning_method='batch')\n",
    "lda.fit(count_data)\n",
    "\n",
    "\n",
    "#\n",
    "# --- NMF ---\n",
    "nmf = NMF(n_components=N_TOPIC, random_state=42)\n",
    "nmf.fit(tfidf_data)\n",
    "\n",
    "def extract_topics(model, vectorizer, top_n=10):\n",
    "    \"\"\"Extract top words from LDA topics\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return [[feature_names[i] for i in topic.argsort()[:-top_n-1:-1]]\n",
    "            for topic in model.components_]\n",
    "# Extract LDA topics\n",
    "lda_topics = extract_topics(lda, count_vectorizer)\n",
    "\n",
    "# Extract NMF topics\n",
    "nmf_topics = extract_topics(nmf, tfidf_vectorizer)\n",
    "\n",
    "#create human-assigned labels\n",
    "lda_labels = [\n",
    "    \"Overall Movie Judgments\",\n",
    "    \"Praising Performances, Music, & Comedy\",\n",
    "    \"Critiquing Male Roles & Horror\",\n",
    "    \"War & Action Films\",\n",
    "    \"Drama, Romance, & Adaptations\"\n",
    "]\n",
    "\n",
    "nmf_labels = [\n",
    "    \"Analyzing Direction & Characters\",\n",
    "    \"Strongly Negative Reviews (Waste of Money)\",\n",
    "    \"Praising Comedy, Music, & Performances\",\n",
    "    \"Discussing Book/Novel Adaptations\",\n",
    "    \"TV Series Fan Discussion (Seasons, Episodes)\"\n",
    "]\n",
    "\n",
    "# Create Pandas DataFrames\n",
    "\n",
    "lda_df = pd.DataFrame({\n",
    "    \"Topic #\": [f\"Topic {i + 1}\" for i in range(len(lda_topics))],\n",
    "    \"Human Label\": lda_labels,\n",
    "    \"Top Words\": [\", \".join(words) for words in lda_topics]\n",
    "})\n",
    "\n",
    "nmf_df = pd.DataFrame({\n",
    "    \"Topic #\": [f\"Topic {i + 1}\" for i in range(len(nmf_topics))],\n",
    "    \"Human Label\": nmf_labels,\n",
    "    \"Top Words\": [\", \".join(words) for words in nmf_topics]\n",
    "})\n",
    "\n",
    "#print the DataFrames\n",
    "#set display options to show full text in columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" LDA Topics \" + \"=\"*30)\n",
    "print(lda_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" NMF Topics \" + \"=\"*30)\n",
    "print(nmf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31ab2d-32ba-4a32-88eb-f3d731c59054",
   "metadata": {},
   "source": [
    "## PART 4 — EVALUATION & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b538beef-3230-4bf8-b706-57a102cc14d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:52:46.588873Z",
     "start_time": "2025-10-27T19:52:35.962988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Coherence Scores...\n",
      "\n",
      "============================== Coherence Score Comparison ==============================\n",
      "LDA C_v = 0.3749, NMF C_v = 0.4696\n",
      "Result: NMF produced slightly more coherent topics.\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute Topic Coherence (C_v) \n",
    "\n",
    "print(\"Calculating Coherence Scores...\")\n",
    "\n",
    "# Create the tokenized list of lists needed for CoherenceModel\n",
    "processed_texts = [text.split() for text in df['clean_review']]\n",
    "\n",
    "# Create a Gensim Dictionary from our tokenized texts\n",
    "dictionary = Dictionary(processed_texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "id2word = dictionary\n",
    "\n",
    "#define function to compute topic coherence ---\n",
    "def topic_coherence(topics, texts, dictionary):\n",
    "    \"\"\"Compute coherence score\"\"\"\n",
    "    cm = CoherenceModel(topics=topics,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v'\n",
    "                        )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "# Calculate LDA Coherence\n",
    "coherence_lda = topic_coherence(lda_topics, processed_texts, dictionary)\n",
    "\n",
    "# Calculate NMF Coherence\n",
    "coherence_nmf = topic_coherence(nmf_topics, processed_texts, dictionary)\n",
    "\n",
    "# Report and Compare\n",
    "print(\"\\n\" + \"=\"*30 + \" Coherence Score Comparison \" + \"=\"*30)\n",
    "print(f\"LDA C_v = {coherence_lda:.4f}, NMF C_v = {coherence_nmf:.4f}\")\n",
    "\n",
    "if coherence_nmf > coherence_lda:\n",
    "    print(\"Result: NMF produced slightly more coherent topics.\")\n",
    "elif coherence_lda > coherence_nmf:\n",
    "    print(\"Result: LDA produced slightly more coherent topics.\")\n",
    "else:\n",
    "    print(\"Result: Both models produced topics with identical coherence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c7c9b7803d9cd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:53:13.263090Z",
     "start_time": "2025-10-27T19:52:46.613864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing LDA visualization for Jupyter/Colab...\n",
      "Displaying visualization...\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# --- 2. Visualize Topics (pyLDAvis – LDA only) ---\n",
    "\n",
    "print(\"\\nPreparing LDA visualization for Jupyter/Colab...\")\n",
    "\n",
    "# Enable the visualization notebook mode\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization data using the consolidated lda_model method\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=N_TOPIC,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto'\n",
    ")\n",
    "\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "print(\"Displaying visualization...\")\n",
    "\n",
    "# Display the visualization\n",
    "vis\n",
    "\n",
    "#export visualization as an HTML screenshot\n",
    "pyLDAvis.save_html(vis, \"lda_visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e295bbc8-2771-4b88-9907-de8db2f1af36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:53:13.311285Z",
     "start_time": "2025-10-27T19:53:13.308555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Interpretation of visualization \n",
    "#The LDA visualization shows 5 topics: the two largest topics (1 & 2) overlap heavily, \n",
    "#while topics 3 & 4 form a separate cluster and topic 5 is distinct.\n",
    "#The themes relate to film and acting, but this overlap indicates lower coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb5e4b-2ab6-4c08-859e-ca913c50e0ac",
   "metadata": {},
   "source": [
    "## PART 5 — PREPROCESSING VARIANTS STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f22f67-de7d-4248-96ff-12ab06b9b1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:57:33.324752Z",
     "start_time": "2025-10-27T19:53:13.396757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== Pipeline ==============================\n",
      "\n",
      "Running pipeline: Lemmatization + Remove StopWords\n",
      "\n",
      " Finish pre-processing\n",
      "\n",
      " Finish sentiment analysis\n",
      "\n",
      " Finish coherence model\n",
      "\n",
      " Finish train Model\n",
      "\n",
      "============================== Pipeline ==============================\n",
      "\n",
      "Running pipeline: Stemming + Keep StopWords\n",
      "\n",
      " Finish pre-processing\n",
      "\n",
      " Finish sentiment analysis\n",
      "\n",
      " Finish coherence model\n",
      "\n",
      " Finish train Model\n",
      "\n",
      "============================== Pipeline ==============================\n",
      "\n",
      "Running pipeline: Unigrams + Bigrams\n",
      "\n",
      " Finish pre-processing\n",
      "\n",
      " Finish sentiment analysis\n",
      "\n",
      " Finish coherence model\n",
      "\n",
      " Finish train Model\n",
      "\n",
      "Sentiment Analysis table\n",
      "                           Pipeline  Accuracy    Recall  Precision  F1-score\n",
      "0  Lemmatization + Remove StopWords     0.874  0.905095   0.852305  0.877907\n",
      "1         Stemming + Keep StopWords     0.871  0.900100   0.850803  0.874757\n",
      "2                Unigrams + Bigrams     0.877  0.908092   0.855127  0.880814\n",
      "\n",
      " Coherence Table\n",
      "                           Pipeline  lda_coherence  nmf_coherence\n",
      "0  Lemmatization + Remove StopWords       0.389678       0.469617\n",
      "1         Stemming + Keep StopWords       0.177750       0.242114\n",
      "2                Unigrams + Bigrams       0.377932       0.533920\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import time\n",
    "import psutil\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines ---\n",
    "\n",
    "#Pipeline 1: Lemmatization + Remove StopWords\n",
    "def clean_lemmatize(text):\n",
    "    text = base_clean(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in custom_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "#Pipeline 2: Stemming + Keep StopWords\n",
    "stemmer = PorterStemmer()\n",
    "def clean_stem(text):\n",
    "    text = base_clean(text)\n",
    "    tokens = [stemmer.stem(w) for w in text.split()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "#Pipeline 3: Unigrams + Bigrams\n",
    "def clean_bigram(text):\n",
    "    text = base_clean(text)  # your existing normalization: lower, strip HTML, etc.\n",
    "    # Basic tokenization (adjust to your base_clean)\n",
    "    tokens = [w for w in text.split() if w and w not in custom_stopwords]\n",
    "\n",
    "    # Build bigrams\n",
    "    bigrams = [\"_\".join(pair) for pair in zip(tokens, tokens[1:])]\n",
    "\n",
    "    # Return BOTH unigrams and bigrams\n",
    "    return \" \".join(tokens + bigrams)\n",
    "\n",
    "\n",
    "pipelines = {\n",
    "    \"Lemmatization + Remove StopWords\": clean_lemmatize,\n",
    "    \"Stemming + Keep StopWords\": clean_stem,\n",
    "    \"Unigrams + Bigrams\": clean_bigram\n",
    "}\n",
    "\n",
    "# --- Reusable evaluation functions ---\n",
    "\n",
    "\n",
    "# Function to compute topic diversity \n",
    "def topic_diversity(topics, top_n=10):\n",
    "    \"\"\"Compute diversity score\"\"\"\n",
    "    all_words = [w for topic in topics for w in topic[:top_n]]\n",
    "    return len(set(all_words)) / len(all_words) if all_words else 0\n",
    "\n",
    "# Function to compute topic stability\n",
    "def jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if (set1 | set2) else 0.0\n",
    " \n",
    "def compute_stability(count_data, feature_names, seeds=[10, 42, 100], n_topics=N_TOPIC, top_k=10):\n",
    "    \"\"\"Compute topic stability across multiple runs\"\"\"\n",
    "    runs = []\n",
    "    for rs in seeds:\n",
    "        lda = LatentDirichletAllocation(n_components=N_TOPIC, random_state=rs, max_iter=5)\n",
    "        lda.fit(count_data)\n",
    "        topic_sets = [set(feature_names[i] for i in topic.argsort()[:-top_k-1:-1])\n",
    "                     for topic in lda.components_]\n",
    "        runs.append(topic_sets)\n",
    "\n",
    "    # Average Jaccard across all pairs\n",
    "    scores = [sum(jaccard_similarity(runs[i][k], runs[j][k]) for k in range(n_topics)) / n_topics\n",
    "             for i, j in combinations(range(len(runs)), 2)]\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "\n",
    "# --- Storage for results ---\n",
    "results_Analysis = []\n",
    "results_coherence = []\n",
    "results = []\n",
    "for name, func in pipelines.items():\n",
    "    print(\"\\n\" + \"=\"*30 + \" Pipeline \" + \"=\"*30)\n",
    "    print(f\"\\nRunning pipeline: {name}\")\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process()\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df_variant = df.copy()\n",
    "    df_variant['clean_review'] = df_variant['review'].apply(func)\n",
    "    print(\"\\n Finish pre-processing\")\n",
    "\n",
    "    # --- Sentiment Analysis ---\n",
    "    tfidf_vectorizer_variant = TfidfVectorizer(max_df=0.95, min_df=5)\n",
    "    X = tfidf_vectorizer_variant.fit_transform(df_variant['clean_review'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, df_variant['sentiment'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a NEW model for each variant\n",
    "    model_variant = LogisticRegression(max_iter=1000)\n",
    "    model_variant.fit(X_train, y_train)\n",
    "    y_pred = model_variant.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='positive')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='positive')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='positive')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_data, df['sentiment'], test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"\\n Finish sentiment analysis\")\n",
    "    results_Analysis.append({\n",
    "        \"Pipeline\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"F1-score\": f1,\n",
    "    })\n",
    "\n",
    "# Topic Modeling\n",
    "    count_vectorizer_variant = CountVectorizer(max_df=0.95, min_df=5)\n",
    "    if name == \"Unigrams + Bigrams\":\n",
    "        count_vectorizer_variant = CountVectorizer(max_df=0.95, min_df=5, ngram_range=(1,2))\n",
    "    tfidf_vectorizer_variant = TfidfVectorizer(max_df=0.95, min_df=5)\n",
    "    if name == \"Unigrams + Bigrams\":\n",
    "        tfidf_vectorizer_variant = TfidfVectorizer(max_df=0.95, min_df=5, ngram_range=(1,2))\n",
    "\n",
    "    count_data_variant = count_vectorizer_variant.fit_transform(df_variant['clean_review'])\n",
    "    tfidf_data_variant = tfidf_vectorizer_variant.fit_transform(df_variant['clean_review'])\n",
    "\n",
    "    lda_variant = LatentDirichletAllocation(n_components=N_TOPIC, random_state=42)\n",
    "    lda_variant.fit(count_data_variant)\n",
    "\n",
    "    nmf_variant = NMF(n_components=N_TOPIC, random_state=42, max_iter=600)\n",
    "    nmf_variant.fit(tfidf_data_variant)\n",
    "\n",
    "    feature_names_variant = count_vectorizer_variant.get_feature_names_out()\n",
    "    lda_topics_variant = extract_topics(lda_variant, count_vectorizer_variant)\n",
    "    nmf_topics_variant = extract_topics(nmf_variant, tfidf_vectorizer_variant)\n",
    "\n",
    "    # Create the tokenized list of lists needed for CoherenceModel\n",
    "    processed_texts = [text.split() for text in df['clean_review']]\n",
    "    # Create a Gensim Dictionary from our tokenized texts\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "\n",
    "    lda_coherence = topic_coherence(lda_topics_variant, processed_texts, dictionary)\n",
    "    nmf_coherence = topic_coherence(nmf_topics_variant, processed_texts, dictionary)\n",
    "    print(\"\\n Finish coherence model\")\n",
    "    # --- Topic Modeling ---\n",
    "    results_coherence.append({\n",
    "        \"Pipeline\": name,\n",
    "        \"lda_coherence\": lda_coherence,\n",
    "        \"nmf_coherence\": nmf_coherence,\n",
    "    })\n",
    "\n",
    "    ## ------ Coherence --------\n",
    "    coherence = 0\n",
    "    if nmf_coherence > lda_coherence:\n",
    "        coherence = nmf_coherence\n",
    "    elif lda_coherence > nmf_coherence:\n",
    "        coherence = lda_coherence\n",
    "\n",
    "    ## ------ Diversity --------\n",
    "    diversity = topic_diversity(lda_topics_variant)\n",
    "\n",
    "    ## ------ Stability --------\n",
    "    stability = compute_stability(count_data_variant, feature_names_variant)\n",
    "    \n",
    "    ## ------ Run Time --------\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    ## ------ Memory Usage --------\n",
    "    memory_used = process.memory_info().rss / (1024*1024)\n",
    "\n",
    "    print(\"\\n Finish train Model\")\n",
    "\n",
    "    results.append({\n",
    "        \"Pipeline\": name,\n",
    "        \"F1\": f1,\n",
    "        \"Coherence\": coherence,\n",
    "        \"Diversity\": diversity,\n",
    "        \"Stability\": stability,\n",
    "        \"Runtime (s)\": runtime,\n",
    "        \"Memory (MB)\": memory_used\n",
    "    })\n",
    "\n",
    "# --- Summarize results ---\n",
    "results_df = pd.DataFrame(results)\n",
    "results_Analysis_df = pd.DataFrame(results_Analysis)\n",
    "results_coherence_df = pd.DataFrame(results_coherence)\n",
    "print (\"\\nSentiment Analysis table\")\n",
    "print (results_Analysis_df)\n",
    "print (\"\\n Coherence Table\")\n",
    "print (results_coherence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c1ed2-d2f1-48e5-b004-b8f0661a5050",
   "metadata": {},
   "source": [
    "## PART 6 — PREPROCESSING QUALITY INDEX (PQI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d288f887-2765-4dbd-8b3b-2d40337a61c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:57:33.416335Z",
     "start_time": "2025-10-27T19:57:33.402679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results with PQI:\n",
      "                           Pipeline      F1  Coherence  Diversity  Stability  \\\n",
      "0  Lemmatization + Remove StopWords  0.8779     0.4696       0.70     0.2213   \n",
      "1         Stemming + Keep StopWords  0.8748     0.2421       0.40     0.5612   \n",
      "2                Unigrams + Bigrams  0.8808     0.5339       0.52     0.4086   \n",
      "\n",
      "   Runtime (s)  Memory (MB)     PQI  \n",
      "0      76.4598     475.3594  0.3509  \n",
      "1     106.8254     557.0938  0.1110  \n",
      "2      76.6147     846.1875  0.5932  \n",
      "\n",
      "Best Pipeline based on PQI: Unigrams + Bigrams\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Min-max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Create a new dataframe for normalized values\n",
    "df_norm = pd.DataFrame()\n",
    "\n",
    "# F1, Coherence, etc., are 'good': higher values are better.\n",
    "good_cols = ['F1', 'Coherence', 'Diversity', 'Stability']\n",
    "\n",
    "# Scale the 'good' columns (0=worst, 1=best)\n",
    "df_norm[good_cols] = scaler.fit_transform(results_df[good_cols])\n",
    "\n",
    "# Runtime and Memory are 'costs': lower values are better.\n",
    "bad_cols = ['Runtime (s)', 'Memory (MB)']\n",
    "\n",
    "# Scale the 'bad' columns (0=worst, 1=best)\n",
    "df_norm[bad_cols] = 1 - scaler.fit_transform(results_df[bad_cols])\n",
    "\n",
    "# PQI formula\n",
    "results_df[\"PQI\"] = (\n",
    "    0.30 * df_norm[\"F1\"] +\n",
    "\n",
    "    0.25 * df_norm[\"Coherence\"] +\n",
    "    0.15 * df_norm[\"Diversity\"] +\n",
    "    0.15 * df_norm[\"Stability\"] -\n",
    "    0.10 * df_norm[\"Runtime (s)\"] -  \n",
    "    0.05 * df_norm[\"Memory (MB)\"]  \n",
    ")\n",
    "\n",
    "best_pipeline = results_df.loc[results_df[\"PQI\"].idxmax(), \"Pipeline\"]\n",
    "\n",
    "print(\"\\nFinal Results with PQI:\")\n",
    "print(results_df.round(4))\n",
    "print(f\"\\nBest Pipeline based on PQI: {best_pipeline}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
